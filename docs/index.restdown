---
title: Manatee
markdown2extras: wiki-tables, code-friendly
apisections:
---

# Manatee - Postgres HA Cluster.

This document describes the high level design for Manatee - A system built on
top of Postgres synchronous replication to deliver a Postgres cluster that is
consistent whilst maximizing availability.

# Background

It's assumed readers are familiar with Zookeeper and Postgres. Read through
[this](http://zookeeper.apache.org/doc/trunk/zookeeperOver.html) Zookeeper guide
before starting this document.

Postgres offers synchronous and asynchronous streaming replication. For a
detailed look, check out the Postgres
[docs](http://www.Postgresql.org/docs/9.1/interactive/warm-standby.html#SYNCHRONOUS-REPLICATION)
on this subject matter.

## Brief Overview of Postgres Synchronous Replication

In this scheme, there are 2 Postgres peers, a primary and a standby. The standby
operates in read-only mode whilst the primary takes all of the writes. Each
commit of a write transaction will wait until confirmation is received that the
commit has been written to the transaction log of both the primary and the standby.

Responses to the client on writes made to the primary will wait until the
standby responds. The response may never occur if the standby crashes.
Regardless of whether the standby responds, the record will be written to the
primary. From the point of view of the client, the response is
indistinguishable from a 500, and the client will not know for certain whether
the write request has succeeded.

Basically, the default Postgres implementation of synchronous replication
results in inconsistency between the client and Postgres on standby failures.
Postgres' solution to this is achieved by adding multiple potential synchronous
standbys.  The first one in the list will be used as the synchronous standby.
Standbys listed after this will take over as the synchronous standby should the
first one fail.

This default setup is insufficient when the primary fails.
Postgres does not provide automatic primary failover, which results in the loss
of write availability until an operator can be engaged.

## The Number 3

If a Postgres Shard is provisioned with 3 Peers, one primary,
one synchronous standby and one asynchronous standby, the Shard can tolerate the
loss of any 1 Peer without becoming unavailable. Unfortunately, adding more
Peers past 3 to a Shard does not increase the availability of a Shard. If at
anytime, the primary and synchronous standby become unavailable, regardless of
the number of the Peers, the Shard becomes unavailable for writes but still
available for reads as all other peers are asynchronous standbys and thus
cannot be guaranteed to be up to date.

# Design Goals

A Postgres shard consisting of 3 peers must be able to:

1. Provide group membership discovery to clients.
2. Remain available to writes/reads in the face of failure of any 1 peer -
Specifically, re-assigning the role of the primary to the standby peer in the
face of primary failure.
3. Automate peer recovery - If a peer is down for an amount of time that exceeds
the WAL cache, backup/restore from another peer should be automatic.

With these goals in mind, we can ensure Manatee will be available, consistent,
and partition tolerant as long as each shard doesn't lose more than 1 out of its
3 peers.

All bets are off if we lose more than 1. In this case, the shard will remain in
read-only mode until another peer comes back.

# Overview

Manatee consists of the following logical components:

- Zookeeper.
- Postgres.
- Backup/Restore agent.
- Postgres Sitter.

# Zookeeper

Zookeeper is used for the following tasks:

1. Liveliness checks of Peers within a shard.
2. Primary election of Peers within a shard.
3. Discovery of Shards by Manatee clients.

Tasks 1 and 2 are described in the Postgres Sitter section, whilst task 3 is
described in the Registrar Service section.

# Shards and Peers

A Postgres Shard in Manatee is uniquely identified by a configurable url,
generally something like:

    1.moray.us-sw-1.joyent.com

The Shard consists of 3 mirrored Postgres instances, henceforth known as Peers.
Each Peer registers itself with an **emphemeral** node under this path in zookeeper:

    /manatee/1.moray.us-sw-1.joyent.com/

This node is used for leader election to determine the primary peer of the shard.

Shard membership is stored under an emphemeral znode under a configured path:

    /com/joyent/us-sw-1/moray/1/pg

This path is configurable, but generally shared with the binder path of the moray
shard that postgres is backing.

Each shard entry contains the following JSON object:

    {
      "type": "database",
      "database": {
        "primary": "tcp://postgres@10.99.99.222:5432/postgres",
        "sync": "tcp://postgres@10.99.99.37:5432/postgres",
        "async": "tcp://postgres@10.99.99.89:5432/postgres",
        "backupUrl": "http://10.99.99.222:12345",
        "primaryPath": "/shard/1.moray.us-sw-1.joyent.com/shard-0000000023"
      },
      "ttl": 10
    }

Only the database object is used by manatee, the other objects in the shard
entry are used by the binder for dns purposes.
The shard entry is updated/created by the current primary in the shard. As group
memberships change, the entry will be updated accordingly.

# Restore Service

This is an internal REST service that allows peers in each shard to recover or
bootstrap themselves from another peer in the service.  This service is only
used when the recovery of a peer from just WAL is not possible. e.g. database
corruption, WAL cache exhaustion, or whilst bootstrapping a new peer. The backup
and restore mechanism relies on ZFS snapshots of the db data dir, and utilizes
@JWilsdon's ZFS send/recv lib for transport.

This service is made up of the following components on each Postgres Peer:

- A snapshot agent that takes periodic ZFS snapshots of the db dir.
- A REST service that takes backup requests.
- A restore agent that sends the snapshot to the requestor.

The backup process is asynchronous:

1. Client POSTS to /backup/ to indicate a backup request.
2. The service creates /backup/uuid and returns this URI to the client.
3. The service initiates ZFS send/recv to the client.
4. The client polls /backup/uuid for the status of the backup.
5. Once the backup has successfully completed, the service updates
/backup/uuid to done status.
6. The client polls /backup/uuid and discovers the backup has finished
succesfully.

In practice, backup requests will only be sent to the primary peer of a shard.

# Postgres Sitter

## Postgres Replication Recap

It's important to note that the change of a Postgres' instance's replication
state i.e. from standby to primary, or primary to standby, requires a restart
of the Postgres instance.  In addition, the primary Postgres instance requires
the URLs of all standby hosts, and each standby host requires the URL of the
primary.  The distinction of whether a standby is synchronous or asynchronous
is soley made on the primary as the standbys have the same configuration
regardless of their replication mode.

## Overview

The Postgres sitter is an agent co-located with each Postgres Peer.

The sitter is responsible for:

- Replication role (primary, standby) determination.
- Postgres initialization.
- Start/stop Postgres.
- Restore/bootstrap a corrupt Postgres Peer.
- Postgres health check.
- Primary failover.
- Primary leader election.

The Postgres instance can only be started/stopped by the Sitter, at no time can
Postgres start/stop on its own. The Sitter is only changing replication role
state of the peers in the event of a primary failure, and not in the event of
standby failures. This is described in further detail in the failure section.

The responsibilities of the Sitter is best described via examples.

## Bootstrapping 3 new Peers into a Shard

Let's call the 3 sitters A, B, and C which belong to shard
1.moray.us-sw-1.joyent.com. Bootstrapping occurs this way:

- The standard [zookeeper election algorithm](http://zookeeper.apache.org/doc/trunk/recipes.html#sc_leaderElection) is executed on all 3 sitters.
- The leader becomes primary.
- The remaining two peers become the sync and async standbys, depending on who
had the smaller sequence.
- The primary initializes its Postgres instance.
- The standbys initiate db restoration by calling the Restore service.
- After successful restores, the standbys start their Postgres instance.
- The primary checks the status of replication via the pg\_stat\_replication table.
- If replication has been successfully setup, the primary publishes group
membership to the Registrar service. This indicates to clients that this Shard
is ready to take writes.

## Peer Coordination

### Leader Discovery

Manatee uses the [ZKPlus Generic Election Algorithm](https://github.com/mcavage/node-zkplus/blob/master/lib/generic-election.js).
Each peer creates an ephemeral and sequence znode under the path

    /manatee/1.moray.us-sw-1.joyent.com/election/

with their Postgres url as the data. This results in the following state in
Zookeeper:

    /manatee/1.moray.us-sw-1.joyent.com/election/$ip-00000 -> {A.ip, A.zoneId}
    /manatee/1.moray.us-sw-1.joyent.com/election/$ip-00001 -> {B.ip, B.zoneid}
    /manatee/1.moray.us-sw-1.joyent.com/election/$ip-00002 -> {C.ip, C.zoneid}

It's best to think of this election as a singly linked list, where the tail of
the list is the overal leader of the shard and has the lowest sequence number.
Each element is only aware of its immediate leader and not the entire
electorate. The only difference is the overall leader is aware that it's the
overall leader of the shard.

    +-------+     +-------+     +-------+
    |       |     |       |     |       |
    |   C   |     |   B   |     |  A    |
    |       +---->|       +---->|leader |
    |async  |     | sync  |     |primary|
    +-------+     +-------+     +-------+

When a node leaves the shard (say due to a network partition) its child node
will notified that the next available node in the list is its master.

### Follower Discovery
A leader discovers its follower via heartbeats sent from the follower. When a
follower dies, the heartbeat will eventually expire, and the leader will be
notified of the death.

## Node Failure
Each shard can tolerate failures of any single node in the shard and continue
to take writes.

### Synchronous Standby Fails
Given the previous figure, if B dies, its heartbeats to A will expire, and A
will be notified of B's death. C will also be notified that it has a new
leader, A via ZK, and will heartbeat to A, and the shard looks like this:

    +-------+     +-------+     +-------+
    |       |     |       |     |       |
    |   B   |     |   C   |     |  A    |
    | dead  +     |       +---->|leader |
    |       |     | sync  |     |primary|
    +-------+     +-------+     +-------+

A will start synchronously streaming to C, in affect promoting C's replication
from async to sync,  and postgres writes can continue.

### Asynchronous Standby Fails
If C fails, it will stop healthchecking to B, and B will expire C as a follower.
Interactions between A and B are unchanged.

    +-------+     +-------+     +-------+
    |       |     |       |     |       |
    |   C   |     |   B   |     |  A    |
    | dead  +     |       +---->|leader |
    |       |     | sync  |     |primary|
    +-------+     +-------+     +-------+

### Primary Fails
If A fails, B will receive a notification that it is now the overall leader of
the shard, it will become the primary, and switch replication state from async
to sync.

    +-------+     +-------+     +-------+
    |       |     |       |     |       |
    |   C   |     |   B   |     |  A    |
    |       +---->|leader +     |       |
    | sync  |     |primary|     | dead  |
    +-------+     +-------+     +-------+

### >1 Node Fails

If more than 1 node fails at the same time (perhaps due to a ZK outage), it's
critical the async must not become the primary. Since the async will never be
completely up to date, if it becomes the primary, and the other nodes later
come back, they will not be able to replicate as their postgres transaction
logs(xlogs) will have diverged.

This is also true when any node goes away and later comes back as the
primary. There could have been another primary in the shard that had been
taking writes, which would result in the divergence of xlogs between nodes in
the shard, which will cause data corruption.

Manatee has two facilities which will prevent data corruption. xlog
and transition checks.

#### xlog Checks

Whenever a new standby connects to the leader of the shard, the leader will
always check its replication state and ensure that the standby's xlogs are in
sync.  This ensures no divergence in xlog state between the leader and its
standby. If the replication check fails, then it is not safe to accept writes
to the shard, and the leader will put the entire shard in a degraded state. In
this degraded state, all nodes will be read only and operator intervention is
required to repair the shard.

#### Transition Checks

A node must pass the following conditions before it can become the leader of
the shard. These conditions are centered around the last known state of the
node, which will either be primary, sync, or async.

- The state must have not been async.
- If the state was primary, then the last known leader of the shard must be
  this node.
- If the state was sync, then the last know leader of the shard must be the
  last known leader of this node.

These conditions will prevent a node from becoming the leader of the shard if
there is a possibility of xlog divergence. Instead, the node will wait for
others to join the shard before rejoining the election, and reevaulating its
state.

# Operator Tools

## Manatee-stat

Manatee-stat returns the current state of the manatee shard. It will print out
all of the nodes of the shard, and decorate it with their perspective roles and
state.

    $ ./bin/manatee-stat -p $ZK_IP
    {
            "1.moray.coal.joyent.us": {
                "primary": {
                    "ip": "10.77.77.8",
                    "pgUrl": "tcp://postgres@10.77.77.8:5432/postgres",
                    "zoneId": "148ee288-951a-468c-9927-9e4d1010f2bd",
                    "repl": {
                        "pid": 68912,
                        "usesysid": 10,
                        "usename": "postgres",
                        "application_name": "tcp://postgres@10.77.77.7:5432/postgres",
                        "client_addr": "10.77.77.7",
                        "client_hostname": "",
                        "client_port": 65459,
                        "backend_start": "2013-11-21T07:29:26.299Z",
                        "state": "streaming",
                        "sent_location": "0/A000000",
                        "write_location": "0/A000000",
                        "flush_location": "0/A000000",
                        "replay_location": "0/A000000",
                        "sync_priority": 1,
                        "sync_state": "sync"
                    }
                },
                "sync": {
                    "ip": "10.77.77.7",
                    "pgUrl": "tcp://postgres@10.77.77.7:5432/postgres",
                    "zoneId": "dac030aa-3ae9-4b43-8a75-dcdcdf3613b9",
                    "repl": {
                        "pid": 68900,
                        "usesysid": 10,
                        "usename": "postgres",
                        "application_name": "tcp://postgres@10.77.77.9:5432/postgres",
                        "client_addr": "10.77.77.9",
                        "client_hostname": "",
                        "client_port": 37598,
                        "backend_start": "2013-11-21T07:29:21.653Z",
                        "state": "streaming",
                        "sent_location": "0/A000000",
                        "write_location": "0/A000000",
                        "flush_location": "0/A000000",
                        "replay_location": "0/A000000",
                        "sync_priority": 0,
                        "sync_state": "async"
                    }
                },
                "async": {
                    "ip": "10.77.77.9",
                    "pgUrl": "tcp://postgres@10.77.77.9:5432/postgres",
                    "zoneId": "79d29690-b414-4aef-8f28-cdac5860d353",
                    "repl": {},
                    "lag": {
                        "time_lag": null
                    }
                },
                "registrar": {
                    "type": "database",
                    "database": {
                        "primary": "tcp://postgres@10.77.77.8:5432/postgres",
                        "ttl": 60
                    }
                }
            }
        }

The "repl" field shows the postgres replication state of that node. A lack of
this field for the primary or sync almost always means that postgres
replication is broken.

## Manatee-history

Manatee-history returns all of the state transitions for a shard.

    $ ./bin/manatee-history 1.moray.coal.joyent.us $ZK_IPS
    {"time":"1384997168105","ip":"10.77.77.7","action":"AssumeLeader","role":"Leader","master":"","slave":"","zkSeq":"0000000000"}
    {"time":"1384997187193","ip":"10.77.77.8","action":"NewLeader","role":"Standby","master":"10.77.77.7","slave":"","zkSeq":"0000000001"}
    {"time":"1384997189225","ip":"10.77.77.7","action":"NewStandby","role":"Leader","master":"","slave":"10.77.77.8","zkSeq":"0000000002"}
    {"time":"1384997447741","ip":"10.77.77.9","action":"NewLeader","role":"Standby","master":"10.77.77.8","slave":"","zkSeq":"0000000003"}
    {"time":"1384997448754","ip":"10.77.77.8","action":"NewStandby","role":"Standby","master":"10.77.77.7","slave":"10.77.77.9","zkSeq":"0000000004"}
    {"time":"1384997820903","ip":"10.77.77.7","action":"NewLeader","role":"Standby","master":"10.77.77.9","slave":"","zkSeq":"0000000005"}
    {"time":"1384997826406","ip":"10.77.77.8","action":"NewLeader","role":"Standby","master":"10.77.77.7","slave":"","zkSeq":"0000000006"}
    {"time":"1384997827421","ip":"10.77.77.7","action":"NewStandby","role":"Standby","master":"10.77.77.9","slave":"10.77.77.8","zkSeq":"0000000007"}
    {"time":"1384997845020","ip":"10.77.77.9","action":"NewLeader","role":"Standby","master":"10.77.77.8","slave":"","zkSeq":"0000000008"}
    {"time":"1384997845028","ip":"10.77.77.9","action":"NewStandby","role":"Standby","master":"10.77.77.8","slave":"10.77.77.7","zkSeq":"0000000009"}
    {"time":"1384997846034","ip":"10.77.77.8","action":"NewStandby","role":"Standby","master":"10.77.77.7","slave":"10.77.77.9","zkSeq":"0000000010"}
    {"time":"1384997846365","ip":"10.77.77.7","action":"AssumeLeader","role":"Leader","master":"","slave":"10.77.77.8","zkSeq":"0000000011"}
    {"time":"1384997910908","ip":"10.77.77.9","action":"ExpiredStandby","role":"Standby","master":"10.77.77.8","slave":"10.77.77.7","zkSeq":"0000000012"}
    {"time":"1385001204609","ip":"10.77.77.8","action":"NewLeader","role":"Standby","master":"10.77.77.9","slave":"","zkSeq":"0000000013"}
    {"time":"1385001204651","ip":"10.77.77.7","action":"NewLeader","role":"Standby","master":"10.77.77.8","slave":"","zkSeq":"0000000014"}
    {"time":"1385001204710","ip":"10.77.77.9","action":"NewLeader","role":"Standby","master":"10.77.77.7","slave":"10.77.77.7","zkSeq":"0000000015"}
    {"time":"1385001205612","ip":"10.77.77.9","action":"NewStandby","role":"Standby","master":"10.77.77.7","slave":"10.77.77.8","zkSeq":"0000000016"}
    {"time":"1385001205630","ip":"10.77.77.8","action":"NewStandby","role":"Leader","master":"10.77.77.9","slave":"10.77.77.9","zkSeq":"0000000017"}
    {"time":"1385018961860","ip":"10.77.77.7","action":"NewLeader","role":"Standby","master":"10.77.77.8","slave":"","zkSeq":"0000000018"}
    {"time":"1385018962229","ip":"10.77.77.9","action":"NewLeader","role":"Standby","master":"10.77.77.7","slave":"","zkSeq":"0000000019"}
    {"time":"1385018962271","ip":"10.77.77.8","action":"AssumeLeader","role":"Leader","master":"","slave":"","zkSeq":"0000000020"}
    {"time":"1385018963243","ip":"10.77.77.7","action":"NewStandby","role":"Standby","master":"10.77.77.8","slave":"10.77.77.9","zkSeq":"0000000021"}
    {"time":"1385018966904","ip":"10.77.77.8","action":"NewStandby","role":"Leader","master":"","slave":"10.77.77.7","zkSeq":"0000000022"}

The output is sorted by time, and each line represents a state transition of a node in the shard.
